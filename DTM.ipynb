{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DTM.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdQYMDGHtEfb","executionInfo":{"status":"ok","timestamp":1638072171511,"user_tz":480,"elapsed":35333,"user":{"displayName":"LANG CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02296514908425475891"}},"outputId":"46acd00a-df0a-4f8c-e36b-8c54c92e95cc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"3zXwXGaLtlQR"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vx9wZFhjukvc"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"CkZFg1-ttSVb"},"source":["train = pd.read_csv(\"drive/Shared drives/MATH156 Project/Data/train.csv\")\n","test = pd.read_csv(\"drive/Shared drives/MATH156 Project/Data/test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a7tGp7GtwEI"},"source":["X_train = train['review'].tolist()\n","y_train = train['sentiment'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBt2RJkzuHK_"},"source":["X_test = test['review'].tolist()\n","y_test = test['sentiment'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DhLLozNuLF5"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EYcosW2usLq","executionInfo":{"status":"ok","timestamp":1638072504846,"user_tz":480,"elapsed":1975,"user":{"displayName":"LANG CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02296514908425475891"}},"outputId":"c31bb5a8-5513-45dd-de7d-90188f33d776"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ioT1bX3qwBUA"},"source":["# Vectorization\n","* CountVectorizer\n","* Unigram + Bigram\n","* Remove non-words/numbers\n","* Remove stopwords\n","* Lemmatization"]},{"cell_type":"code","metadata":{"id":"PCwox78MvGlB"},"source":["\"\"\"\n","vectorizer\n","\"\"\"\n","import re\n","import string\n","\n","from nltk import PorterStemmer, WordNetLemmatizer, sent_tokenize, wordpunct_tokenize, pos_tag\n","from nltk.corpus import wordnet, stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","class YelpSentCountVectorizer(CountVectorizer):\n","    def __init__(self, ngram_range=(1, 1),\n","                 remove_nonwords=False, remove_stopwords=False,\n","                 stem=False, lemmatize=False, min_df=1, binary=False):\n","        super().__init__()\n","        self.punct = set(string.punctuation)\n","        self.ngram_range = ngram_range\n","        self.remove_nonwords = remove_nonwords\n","        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n","        self.stemmer = PorterStemmer() if stem else None\n","        self.lemmatizer = WordNetLemmatizer() if lemmatize else None\n","        self.min_df = min_df\n","        self.binary = binary\n","\n","    def lemmatize(self, token, tag):\n","        tag = {\n","            'N': wordnet.NOUN,\n","            'V': wordnet.VERB,\n","            'R': wordnet.ADV,\n","            'J': wordnet.ADJ\n","        }.get(tag[0], wordnet.NOUN)\n","        return self.lemmatizer.lemmatize(token, tag)\n","\n","    def stem(self, token):\n","        return self.stemmer.stem(token)\n","\n","    def build_analyzer(self):\n","        # create the analyzer that will be returned by this method\n","        def analyser(doc):\n","            # Keep only words\n","            doc = re.sub('[^A-Za-z0-9]+', ' ', doc) if self.remove_nonwords else doc\n","            cleaned_tokens = []\n","            # Break the document into sentences\n","            for sent in sent_tokenize(doc):\n","                # Break the sentence into part of speech tagged tokens\n","                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n","                    # Lower case and strip spaces\n","                    token = token.lower()\n","                    token = token.strip()\n","                    # If stopword, ignore token and continue\n","                    if token in self.stop_words:\n","                        continue\n","                    # If punctuation, continue\n","                    if all(char in self.punct for char in token):\n","                        continue\n","                    # Lemmatize/stem the token\n","                    if self.lemmatizer:\n","                        token = self.lemmatize(token, tag)\n","                    elif self.stemmer:\n","                        token = self.stem(token)\n","                    cleaned_tokens.append(token)\n","            # use CountVectorizer's _word_ngrams built in method to extract n-grams\n","            return self._word_ngrams(cleaned_tokens)\n","\n","        return analyser"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxKseYxxuuRG"},"source":["vect = YelpSentCountVectorizer(ngram_range=(1,2),\n","                                        remove_nonwords=True,\n","                                        remove_stopwords=True,\n","                                        stem=False,\n","                                        lemmatize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCoasj4svqrZ","executionInfo":{"status":"ok","timestamp":1638075196055,"user_tz":480,"elapsed":2421985,"user":{"displayName":"LANG CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02296514908425475891"}},"outputId":"c93a59ad-ffdb-4d01-de8e-fb66ff051d15"},"source":["%time cv = vect.fit(X_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 39min 57s, sys: 18.6 s, total: 40min 16s\n","Wall time: 40min 21s\n"]}]},{"cell_type":"code","metadata":{"id":"BvpALUeewOxm"},"source":["X_train_dtm = vect.transform(X_train)\n","X_test_dtm = vect.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEY8Gxd760pl"},"source":["import pickle\n","\n","with open('drive/Shared drives/MATH156 Project/Data/pickles/X_train_dtm.pickle', 'wb') as f:\n","    pickle.dump(X_train_dtm, f)\n","with open('drive/Shared drives/MATH156 Project/Data/pickles/X_test_dtm.pickle', 'wb') as f:\n","    pickle.dump(X_test_dtm, f)"],"execution_count":null,"outputs":[]}]}